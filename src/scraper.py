import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse

def get_text_from_url(url):
    """Auxiliar: Baixa e limpa o texto de uma URL."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove scripts e estilos
        for script in soup(["script", "style", "nav", "footer", "iframe"]):
            script.decompose()
            
        text = soup.get_text(separator=' ')
        
        # Limpa espa√ßos
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        clean_text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return soup, clean_text
    except Exception as e:
        logging.warning(f"Erro ao acessar {url}: {e}")
        return None, ""

def scrape_website(url):
    """
    Scraper Inteligente V2:
    1. Baixa a p√°gina principal.
    2. Se tiver pouco texto, procura links de 'Research', 'Projects', 'Publications', 'Lab'.
    3. Baixa essas sub-p√°ginas e junta o conte√∫do.
    """
    if not isinstance(url, str) or not url.strip():
        return None

    logging.info(f"üîç Scraping: {url}")
    
    # 1. P√°gina Principal
    soup_main, text_main = get_text_from_url(url)
    
    if not soup_main:
        return None

    final_text = f"--- CONTE√öDO DA HOME PAGE ({url}) ---\n{text_main}\n"
    
    # Se j√° tem bastante texto, retorna logo (economizando tempo)
    if len(text_main) > 5000:
        return final_text[:15000]

    # 2. Busca Links Complementares (Heur√≠stica)
    # Palavras-chave que indicam conte√∫do relevante
    keywords = ['research', 'publication', 'project', 'lab', 'group', 'pesquisa', 'projeto']
    
    # Filtros de exclus√£o (Redes Sociais, Arquivos, etc)
    ignore_domains = ['linkedin.com', 'twitter.com', 'x.com', 'facebook.com', 'instagram.com', 'youtube.com', 'google.com', 'researchgate.net']
    ignore_exts = ['.pdf', '.doc', '.docx', '.zip', '.png', '.jpg']
    ignore_terms_text = ['home', 'contact', 'email', 'login', 'sign in', 'back']
    
    visited_links = set()
    extra_content = []
    
    # Detec√ß√£o de "P√°gina Cart√£o de Visita" (Muito curta, exige navega√ß√£o agressiva)
    is_short_page = len(text_main) < 1000
    
    # Encontra todos os links
    links = soup_main.find_all('a', href=True)
    
    found_relevant_links = 0
    max_links = 3 if is_short_page else 2
    
    for link in links:
        if found_relevant_links >= max_links: 
            break
            
        href = link['href']
        text_link = link.get_text().strip().lower()
        full_url = urljoin(url, href)
        parsed = urlparse(full_url)
        
        # --- FILTROS DE SEGURAN√áA ---
        if full_url == url or href.startswith('#') or not text_link: continue
        if any(parsed.netloc.endswith(d) for d in ignore_domains): continue
        if any(parsed.path.lower().endswith(ext) for ext in ignore_exts): continue
        
        should_follow = False
        
        # Regra 1: Tem palavra-chave no texto ou link?
        if any(w in text_link for w in keywords) or any(w in href.lower() for w in keywords):
            should_follow = True
            
        # Regra 2: Se a p√°gina √© curta, segue links que n√£o sejam "Home/Contact" (Link do Lab muitas vezes √© o nome do lab)
        elif is_short_page:
            if len(text_link) > 2 and not any(t in text_link for t in ignore_terms_text):
                 # Evita sair do dom√≠nio se n√£o tiver certeza ABSOLUTA, exceto se for p√°gina de perfil acad√™mico que linka lab externo
                 should_follow = True

        if should_follow:
            # Restri√ß√£o de Dom√≠nio: Relaxada para permitir Labs em dom√≠nios pr√≥prios
            # Mas evitamos navegar na web inteira. Aceitamos se for subdom√≠nio ou se for 'clicado' por keyword.
            
            if full_url in visited_links: continue
                
            logging.info(f"   ‚Ü≥ Aprofundando em: {full_url}")
            _, sub_text = get_text_from_url(full_url)
            
            # S√≥ adiciona se trouxer conte√∫do novo relevante
            if sub_text and len(sub_text) > 200:
                extra_content.append(f"\n--- CONTE√öDO EXTRA ({text_link.upper()}) ---\nLink: {full_url}\n{sub_text}")
                visited_links.add(full_url)
                found_relevant_links += 1

    # Junta tudo
    if extra_content:
        final_text += "\n".join(extra_content)
    
    return final_text[:25000] # Limite aumentado para gemma/gemini
